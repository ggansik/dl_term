{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "#from Tacotron import Tacotron\n",
    "from text import text_to_sequence, symbols\n",
    "import torch\n",
    "from torch.autograd import Variable\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.autograd import Variable\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.data import sampler\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import torchvision.datasets as dset\n",
    "import torchvision.transforms as T\n",
    "\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "class HighwayNet(nn.Module):\n",
    "    \"\"\"\n",
    "        raw_input: (batch_size, Channel, Length)\n",
    "        input: (# of batch, seq_length, 128(input feature))\n",
    "        h * t + x * (1. - t)\n",
    "        output: (# of batch, seq_length, 128(output feature))\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        super(HighwayNet, self).__init__()\n",
    "        \n",
    "        self.H = nn.Linear(128, 128)\n",
    "        self.T = nn.Linear(128, 128)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        h = F.relu(self.H(x))\n",
    "        t = torch.sigmoid(self.T(x))\n",
    "        \n",
    "        output = h * t + x * (1. - t)\n",
    "        return output\n",
    "\n",
    "\n",
    "\n",
    "class EncoderCBHG(nn.Module):\n",
    "    \"\"\"\n",
    "        raw_input: prenet output\n",
    "        input: (batch_size, channels, seq_length)\n",
    "        Conv1D bank - Max Pooling - Conv1D projection - Conv1D Layer\n",
    "        output: (seq_length, batch_size, 2 * hidden_size)\n",
    "    \"\"\"\n",
    "    def __init__(self, K=16):\n",
    "        super(EncoderCBHG, self).__init__()\n",
    "        #-----------------Conv1Dbank-------------------#\n",
    "        self.conv1dBank = nn.ModuleList(\n",
    "            [nn.Conv1d(128, 128, k, stride=1, padding=k//2)\n",
    "            for k in range(1, K+1)]\n",
    "        )\n",
    "        #-----------------Max pooling------------------#\n",
    "        self.maxPool = nn.MaxPool1d(2, stride=1, padding=1)\n",
    "        #---------------Conv1Dprojection---------------#\n",
    "        self.conv1dProjs = nn.ModuleList(\n",
    "            [nn.Conv1d(128 * K, 128, 3, stride=1, padding=1)\n",
    "            ,nn.Conv1d(128, 128, 3, stride=1, padding=1)]\n",
    "        )\n",
    "        #-----------------Highway Net------------------#\n",
    "        self.highwayNet = nn.ModuleList(\n",
    "            [HighwayNet() for _ in range(4)]\n",
    "        )\n",
    "        #--------------Bidirectional GRU---------------#\n",
    "        self.GRU = nn.GRU(128, 128, bidirectional=True, batch_first=True)\n",
    "        #-------------Batch normalization--------------#\n",
    "        self.bn = nn.BatchNorm1d(128)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "            raw_input: (# of batch, seq_length, 128(output feature))\n",
    "            input: (batch_size, channels, seq_length)\n",
    "            Conv1D bank - Max Pooling - Conv1D projection - Conv1D Layer\n",
    "            output: (seq_length, batch_size, 2 * hidden_size)\n",
    "        \"\"\"\n",
    "        x = x.transpose(1, 2)\n",
    "        #-----------------Conv1Dbank-------------------#\n",
    "        stacked = []\n",
    "        for conv1d in self.conv1dBank:\n",
    "            stacked.append(self.bn(conv1d(x)))\n",
    "        stacked = torch.cat(stacked, dim=1)\n",
    "        #-----------------Max pooling------------------#\n",
    "        y = self.maxPool(stacked)\n",
    "        #---------------Conv1Dprojection---------------#\n",
    "        y = self.bn(self.relu(self.conv1dProjs[0](y)))\n",
    "        y = self.bn(self.conv1dProjs[1](y))\n",
    "        #-------------residual connection--------------#\n",
    "        y = y + x\n",
    "        #----------------Highway Net-------------------#\n",
    "        y = y.T(1, 2)\n",
    "        for layer in self.highwayNet:\n",
    "            y = self.relu(layer(y))\n",
    "        #--------------Bidirectional GRU---------------#\n",
    "        y = y.T(0, 1)\n",
    "        y, _ = self.GRU(y)\n",
    "        \n",
    "        return y\n",
    "\n",
    "\n",
    "class Prenet(nn.Module):\n",
    "    \"\"\"\n",
    "        raw_input: encoder input\n",
    "        input: (# of batch, seq_length, 128(output feature))\n",
    "        FC(Dense) - ReLU - Dropout - FC - ReLU - Dropout\n",
    "        output: (# of batch, seq_length, 128(output feature))\n",
    "\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        super(Prenet, self).__init__()\n",
    "        self.layer = nn.ModuleList(\n",
    "            [nn.Linear(256, 256)\n",
    "            ,nn.Linear(256, 128)]\n",
    "        )\n",
    "        self.dropout = nn.Dropout(p=0.5)\n",
    "        \n",
    "    def forward(self, x):\n",
    "\n",
    "        for layer in self.layer:\n",
    "            x = self.dropout(F.relu(layer(x)))\n",
    "        \n",
    "        return x\n",
    "        \n",
    "\n",
    "class Encoder(nn.Module):\n",
    "    \"\"\"\n",
    "        prenet - CBHG\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        super(Encoder, self).__init__()\n",
    "        \n",
    "        self.prenet = Prenet()\n",
    "        self.cbhg = EncoderCBHG()\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.cbhg(self.prenet(x))\n",
    "        \n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "class HighwayNet(nn.Module):\n",
    "    \"\"\"\n",
    "    h * t + x * (1. - t)\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        super(HighwayNet, self).__init__()\n",
    "        \n",
    "        self.H = nn.Linear(128, 128)\n",
    "        self.T = nn.Linear(128, 128)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        h = F.relu(self.H(x))\n",
    "        t = torch.sigmoid(self.T(x))\n",
    "        \n",
    "        output = h * t + x * (1. - t)\n",
    "        return output\n",
    "\n",
    "class DecoderCBHG(nn.Module):\n",
    "    \"\"\"\n",
    "    Conv1D bank - Max Pooling - Conv1D projection - Conv1D Layer\n",
    "    \"\"\"\n",
    "    def __init__(self, K=8):\n",
    "        super(DecoderCBHG, self).__init__()\n",
    "        #conv1d: (batch_size, Channel, Length)\n",
    "        #-----------------Conv1Dbank-------------------#\n",
    "        self.conv1dBank = nn.ModuleList(\n",
    "            [nn.Conv1d(128, 128, k, stride=1, padding=k//2)\n",
    "            for k in range(1, K+1)]\n",
    "        )\n",
    "        #-----------------Max pooling------------------#\n",
    "        self.maxPool = nn.MaxPool1d(2, stride=1, padding=1)\n",
    "        #---------------Conv1Dprojection---------------#\n",
    "        self.conv1dProjs = nn.ModuleList(\n",
    "            [nn.Conv1d(128 * K, 256, 3, stride=1, padding=1)\n",
    "            ,nn.Conv1d(256, 80, 3, stride=1, padding=1)]\n",
    "        )\n",
    "        #-----------------Highway Net------------------#\n",
    "        self.highwayNet = nn.ModuleList(\n",
    "            [HighwayNet() for _ in range(4)]\n",
    "        )\n",
    "        #--------------Bidirectional GRU---------------#\n",
    "        self.GRU = nn.GRU(128, 128, bidirectional=True, batch_first=True)\n",
    "        #-------------Batch normalization--------------#\n",
    "        self.bn = nn.BatchNorm1d(128)\n",
    "        \n",
    "    def forward(self, x):\n",
    "    \n",
    "        x = x.transpose(1, 2) # Shape: (batch_size, channels, seq_length)\n",
    "        #-----------------Conv1Dbank-------------------#\n",
    "        stacked = []\n",
    "        for conv1d in conv1dBank:\n",
    "            stacked.append(self.bn(conv1d(x)))\n",
    "        stacked = torch.cat(stacked, dim=1)\n",
    "        #shape: \n",
    "        #-----------------Max pooling------------------#\n",
    "        y = self.maxPool(stacked)\n",
    "        #---------------Conv1Dprojection---------------#\n",
    "        y = self.bn(self.relu(self.conv1dProjs[0](y)))\n",
    "        y = self.bn(self.conv1dProjs[1](y))\n",
    "        #-------------residual connection--------------#\n",
    "        y = y + x\n",
    "        #----------------Highway Net-------------------#\n",
    "        for layer in self.highwayNet:\n",
    "            y = self.relu(layer(y))\n",
    "        #--------------Bidirectional GRU---------------#\n",
    "        y, _ = self.GRU(y)\n",
    "        \n",
    "        return y\n",
    "\n",
    "             \n",
    "class AttentionWrapper(nn.Module):\n",
    "    def __init__(self, rnn, use_attention):\n",
    "        super(AttentionWrapper, self).__init__()\n",
    "        self.rnn_cell = rnn\n",
    "        self.attention = use_attention\n",
    "        self.projection_for_decoderRNN = nn.Linear(512, 256, bias=False)\n",
    "    def forward(self, memory, decoder_input, cell_hidden):\n",
    "        \"\"\"\n",
    "        memory = (batch_size, encoder_T, dim)\n",
    "        decoder_input = (batch_size, dim)\n",
    "        cell_hidden (previous time step cell state) = (batch, dim)\n",
    "        \"\"\"\n",
    "        batch_size = memory.size(0)\n",
    "        #cell_input = torch.cat((decoder_input, prev_attention), -1) -- why do we have to concat?\n",
    "        cell_input = decoder_input\n",
    "        query = self.rnn_cell(cell_input, cell_hidden)\n",
    "        #feed into attention\n",
    "        attention_weights = self.attention(query, memory)\n",
    "        #make context vector\n",
    "        attention_weights = F.softmax(attention_weights, dim=-1)\n",
    "        context = torch.bmm(attention_weights.view(batch_size, 1, -1), memory).squeeze(1)\n",
    "        out = self.projection_for_decoderRNN(torch.cat([context, query],dim=-1))\n",
    "        return out, query, attention_weights\n",
    "\n",
    "\n",
    "class BahdanauAttention(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(BahdanauAttention, self).__init__()\n",
    "        self.v = nn.Linear(256,1,bias=False)\n",
    "        self.query_layer = nn.Linear(256,256,bias=False)\n",
    "        self.tanh = nn.Tanh()\n",
    "    def forward(self, query, memory):\n",
    "        \"\"\"\n",
    "        query : (batch, 1 ,dim)\n",
    "        \"\"\"\n",
    "        if query.dim() == 2:\n",
    "            query = query.unsqueeze(1)\n",
    "        attention_weight = self.v(self.tanh(self.query_layer(query) + memory))\n",
    "        return attention_weight\n",
    "\n",
    "\n",
    "class Decoder(nn.Module):\n",
    "    def __init__(self, spect_dim, r=2):\n",
    "        super(Decoder, self).__init__()\n",
    "        self.spect_dim = spect_dim\n",
    "        self.r = r\n",
    "        self.prenet = Prenet()\n",
    "        self.attention_RNN = AttentionWrapper(nn.GRUCell(input_size=256, hidden_size =256), BahdanauAttention())\n",
    "        self.decoder_RNN = nn.ModuleList(\n",
    "                            [nn.GRUCell(input_size=256,hidden_size=256) for _ in range(2)])\n",
    "        self.spectro_layer = nn.Linear(256,spect_dim*r,bias=False)\n",
    "        self.epsilon = 0.2\n",
    "        self.maximum_step = 1000\n",
    "        return\n",
    "    \n",
    "    def forward(self, memory, target=None):\n",
    "        \"\"\"\n",
    "        if training time, input is given, else input is decoder outputs\n",
    "        input : \n",
    "            memory (encoder_output) = (batch_size, encoder_T, char_dim)\n",
    "            decoder_input = (batch_size, decoder_T, dim)\n",
    "        output:\n",
    "            \n",
    "        \"\"\"\n",
    "        batch_size = memory.size(0)\n",
    "        test = target is None\n",
    "        decoder_T = 0\n",
    "        \n",
    "        #train data를 r 단위로 묶어준 후 T의 크기를 바꾸어준다.\n",
    "        if not test:\n",
    "            target = target.view(batch_size, target.size(1) // r, -1)\n",
    "            decoder_T = target.size(1)\n",
    "            target = target.transpose(0,1) #for parallelization\n",
    "            \n",
    "        #2단계 decoderRNN 값 저장할 array\n",
    "        decoderRNN_output = [memory.zero_() for _ in range(len(decoder_RNN))] \n",
    "        \n",
    "        #<GO> Frame\n",
    "        current_input = torch.zero([batch_size, self.r*self.spect_dim])\n",
    "        t = 0\n",
    "        targets = []\n",
    "        attention_weights = []\n",
    "        \n",
    "        while (True):\n",
    "            t = t + 1\n",
    "            #prenet\n",
    "            #(B, spect_dim * r)\n",
    "            prenet_output = self.prenet(current_input)\n",
    "            \n",
    "            #attention\n",
    "            #(B, 256)\n",
    "            attention_output, cell_hidden, attention_weight = self.attention(memory, prenet_output, cell_hidden)\n",
    "            \n",
    "            #decoder\n",
    "            #(B, spect_dim * r)\n",
    "            for idx in range(2):\n",
    "                decoderRNN_output[idx] = self.decoder_RNN[idx](attention_output, decoder_output[idx])\n",
    "                decoderRNN_output[idx] += attention_output\n",
    "                attention_output = decoder_output[idx]\n",
    "            \n",
    "            #projection\n",
    "            targetchar =self.spectro_layer(attention_output)\n",
    "            targets += [targetchar]\n",
    "            attention_weights += [attention_weight]\n",
    "            \n",
    "            #check if this target is the end\n",
    "            if test:\n",
    "                if t > 1 and (targetchar<=self.epsilon).all(): break\n",
    "                if t > self.maximum_step: \n",
    "                    print(\"ERROR : Not converge\")\n",
    "                    break\n",
    "            else:\n",
    "                if t >= decoder_T:\n",
    "                    break\n",
    "                    \n",
    "            #change current input\n",
    "            if test:\n",
    "                current_input = targets[-1]\n",
    "            else:\n",
    "                current_input = target[t-1]\n",
    "        \n",
    "        attention_weights = torch.stack(attention_weights).transpose(0,1)\n",
    "        outputs = torch.stack(outputs).transpose(0,1).contiguous()\n",
    "        return outputs, attention_weights\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PostProcessing(nn.Module):\n",
    "    \"\"\"\n",
    "    make post processing data\n",
    "    input : (B, decoder.T, spect_dim)\n",
    "    \"\"\"\n",
    "    def __init__(self, spect_dim):\n",
    "        super(PostProcessing, self).__init__()\n",
    "        self.postcbhg = DecoderCBHG(K=8)\n",
    "        self.linear = nn.Linear(spect_dim * 2, 1025)\n",
    "    def forward(self, batch_size, data):\n",
    "        \"\"\"\n",
    "            make data shape (B, -1, 80)\n",
    "        \"\"\"\n",
    "        data = data.view(batch_size, -1, 80)\n",
    "        output = self.postcbhg(data)\n",
    "        output = self.linear(output)\n",
    "        \n",
    "        return output\n",
    "\n",
    "\n",
    "class Tacotron(nn.Module):\n",
    "    def __init__(self, vocab_num, input_dim=256, spect_dim=80):\n",
    "        super(Tacotron, self).__init__()\n",
    "        self.input_dim = input_dim\n",
    "        self.spect_dim = spect_dim\n",
    "        self.embedding = nn.Embedding(vocab_num, input_dim) #embedding dimension\n",
    "        self.embedding.weight.data.normal_(0,0.3)\n",
    "        self.Encoder = Encoder()\n",
    "        self.Decoder = Decoder(spect_dim, r=2) #write input_dimension\n",
    "        self.Postprocessing = PostProcessing(spect_dim)\n",
    "    def forward(self, inputs, spect_targets=None, r= 5):\n",
    "        \"\"\"\n",
    "        make total model!\n",
    "        input : (B, encoder.T, in_dim)\n",
    "        \n",
    "        \"\"\"\n",
    "        batch_size = inputs.size(0)\n",
    "        memory = self.embedding(inputs)\n",
    "        \n",
    "        #encoding\n",
    "        #(B, encoder.T, input_dim)\n",
    "        memory = self.Encoder(memory)\n",
    "        \n",
    "        #decoding\n",
    "        #(B, encoder.T, mel_dim * r)\n",
    "        decoder_output = self.decoder(memory, spect_targets)\n",
    "        \n",
    "        #postprocessing\n",
    "        #(B, decoder.T, 1025)\n",
    "        decoder_output = decoder_output.view(B, -1, self.spect_dim)\n",
    "        wav_output = self.PostProcessing(batch_size, decoder_output)\n",
    "        \n",
    "        return decoder_output, wav_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def _pad(seq, max_len):\n",
    "    return np.pad(seq, (0, max_len - len(seq)),\n",
    "                  mode='constant', constant_values=0)\n",
    "\n",
    "\n",
    "def test_taco():\n",
    "    B, T_out, D_out = 2, 400, 80\n",
    "    r = 5\n",
    "    T_encoder = T_out // r\n",
    "\n",
    "    texts = [\"Thank you very much.\", \"Hello\"]\n",
    "    seqs = [np.array(text_to_sequence(\n",
    "        t, [\"english_cleaners\"]), dtype=np.int) for t in texts]\n",
    "    input_lengths = np.array([len(s) for s in seqs])\n",
    "    max_len = np.max(input_lengths)\n",
    "    seqs = np.array([_pad(s, max_len) for s in seqs])\n",
    "\n",
    "    x = torch.LongTensor(seqs)\n",
    "    y = torch.rand(B, T_out, D_out)\n",
    "    x = Variable(x)\n",
    "    y = Variable(y)\n",
    "\n",
    "    model = Tacotron(vocab_num=len(symbols))\n",
    "\n",
    "    print(\"Encoder input shape: \", x.size())\n",
    "    print(\"Decoder input shape: \", y.size())\n",
    "    a, b, c = model(x, spect_targets=y)\n",
    "    print(\"Mel shape:\", a.size())\n",
    "    print(\"Linear shape:\", b.size())\n",
    "    print(\"Attention shape:\", c.size())\n",
    "\n",
    "    assert c.size() == (B, T_encoder, max_len)\n",
    "\n",
    "    # Test greddy decoding\n",
    "    a, b, c = model(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Encoder input shape:  torch.Size([2, 21])\n",
      "Decoder input shape:  torch.Size([2, 400, 80])\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "invalid argument 0: Sizes of tensors must match except in dimension 1. Got 21 and 22 in dimension 2 at c:\\programdata\\miniconda3\\conda-bld\\pytorch_1533090623466\\work\\aten\\src\\th\\generic/THTensorMath.cpp:3616",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-28-9bd1e398ab7e>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mtest_taco\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-27-990d3c042df5>\u001b[0m in \u001b[0;36mtest_taco\u001b[1;34m()\u001b[0m\n\u001b[0;32m     26\u001b[0m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Encoder input shape: \"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     27\u001b[0m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Decoder input shape: \"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 28\u001b[1;33m     \u001b[0ma\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mb\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mc\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mspect_targets\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0my\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     29\u001b[0m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Mel shape:\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0ma\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     30\u001b[0m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Linear shape:\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mb\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m    475\u001b[0m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    476\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 477\u001b[1;33m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    478\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    479\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-26-70833dce3d47>\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, inputs, spect_targets, r)\u001b[0m\n\u001b[0;32m     40\u001b[0m         \u001b[1;31m#encoding\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     41\u001b[0m         \u001b[1;31m#(B, encoder.T, input_dim)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 42\u001b[1;33m         \u001b[0mmemory\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mEncoder\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmemory\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     43\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     44\u001b[0m         \u001b[1;31m#decoding\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m    475\u001b[0m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    476\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 477\u001b[1;33m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    478\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    479\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-21-abc9d0d71582>\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m    119\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    120\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 121\u001b[1;33m         \u001b[0mx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcbhg\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mprenet\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    122\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    123\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m    475\u001b[0m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    476\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 477\u001b[1;33m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    478\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    479\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-21-abc9d0d71582>\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m     65\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mconv1d\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconv1dBank\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     66\u001b[0m             \u001b[0mstacked\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mconv1d\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 67\u001b[1;33m         \u001b[0mstacked\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstacked\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     68\u001b[0m         \u001b[1;31m#-----------------Max pooling------------------#\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     69\u001b[0m         \u001b[0my\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmaxPool\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstacked\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mRuntimeError\u001b[0m: invalid argument 0: Sizes of tensors must match except in dimension 1. Got 21 and 22 in dimension 2 at c:\\programdata\\miniconda3\\conda-bld\\pytorch_1533090623466\\work\\aten\\src\\th\\generic/THTensorMath.cpp:3616"
     ]
    }
   ],
   "source": [
    "test_taco()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
